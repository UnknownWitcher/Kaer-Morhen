# TdarRsync 
**Updated:** *07/01/2024*<br />

I needed a script that would do the following
- [x] Download media files from cloud storage to local storage (cache).
- [x] Preserve relative paths.
- [x] Limit total amount of media files on cache at one time.
- [x] Avoid re-downloading the same files.

### ðŸ“‹ HOW IT WORKS
CLOUD âž¡ï¸ RSYNC âž¡ï¸ CACHE âž¡ï¸ TDARR âž¡ï¸ DISK ARRAY

### âš™ï¸ Configuration
| Name  | Example | Details | Optional |
| ------------- | ------------- | ------------- | ------------- |
| source     | /mnt/disks/series | The location we are copying from  | âŒ |
| target[^1]     | /mnt/user/downloads/series | Temporary location we are copying to | âŒ |
| database[^2]   | /mnt/user/downloads | Where do you want the database to be stored  | âŒ |
| subfolder[^3]  | /Title (2024)/Season 01 | Allows you to specify a subfolder witin the source path  | âœ”ï¸ |
| file limit[^4] | 10 | How many files to download before pausing to let tdarr process | âŒ |

**CONFIG EXAMPLE**

```bash
CONFIG="{
	\"source\": \"/mnt/disks/db_films/movies\",
	\"target\": \"/mnt/user/downloads/dropbox/movies/films\",
	\"database\": \"/mnt/user/downloads\",
	\"subfolder\": \"/2 Fast 2 Furious (2003) {tmdb-584}\",
	\"file_limit\": 10
}"
```
```bash
CONFIG="{
	\"source\": \"/mnt/disks/db_shows/series\",
	\"target\": \"/mnt/user/downloads/dropbox/series/shows\",
	\"database\": \"/mnt/user/downloads\",
	\"subfolder\": [
		\"/Arrow (2012) {tvdb-257655}\",
		\"/The Flash (2014) {tvdb-279121}\",
		\"/Constantine (2014) {tvdb-273690}\",
		\"/DC's Legends of Tomorrow (2016) {tvdb-295760}\"
	],
	\"file_limit\": 10
}"
```

[^1]: Target should always be a temporary path, tdarr uses this as its source and then moves the processed file to its final location.
[^2]: Database is the path where we store empty 0 byte duplicates of our files, this is used to identify files we've already downloaded.
[^3]: Subfolder allows you to be more specific about what you want to download from the source path. This **MUST** be a folder to work
and there are no limits to how deep the subfolder goes. **update:** multiple subfolders is now possible.
[^4]: File limit ensures that the cache being used to store these files doesn't get full while tdarr is processing them, i find 10 is a good limit for my internet connection.

### â˜Š Tdarr Library - basic requirements
|     Name     |    Value    | Necessity |
| ------------- | ------------- | ------------- |
| Source         | [target](#%EF%B8%8F-configuration) | Required |
| Folder Watch   | âœ”ï¸ | Required |
| Process Library| âœ”ï¸ | Required |
| Transcodes     | âœ”ï¸ | Recommended |
| Health Checks  | âœ”ï¸ | Recommended |
>**Note:** With the exception of the required settings, everything else is up to you, as long as tdarr processes the files and removes it
from the target directory then this script will work.

**TDARR FLOW**

In my flow I use the `Move To Directory` plugin with `Keep Relative Path` enabled, so that my paths look like this.

**TDARR Input:** `/mnt/media/downloads/dropbox/series` + `/shows/Arrow (2012) {tvdb-257655}/Season 01/Arrow (2012) - S01E01 - Pilot.mkv`

> TDARR REPLACES ORIGINAL BEFORE MOVING...

**TDARR Move:** `/mnt/media/series` + `/shows/Arrow (2012) {tvdb-257655}/Season 01/Arrow (2012) - S01E01 - Pilot.mkv`

>â“˜ Because this script looks through the source path and compares all the files to the database folder, eventually it will take a bit longer than usual for the script to start, I don't think there will be a solution to this in bash and I'm not sure if one is even possible in any other language without storing this data in a proper database (such as sqlite3), so a future version of this script may see me make the move to sqlite3 to try and reduce the time it takes for this to start.
